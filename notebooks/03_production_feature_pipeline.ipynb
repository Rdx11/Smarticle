{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Part 01: Initial Data Scraping and Loading \n",
    "\n",
    "## 🗒️ This notebook is divided in 4 sections:\n",
    "\n",
    "1. Scraping the arXiv website for scientific papers using the arXiv API,\n",
    "2. Performing some basic data cleaning and preprocessing,\n",
    "3. Connect to the Hopsworks feature store,\n",
    "4. Create feature groups and upload them to the feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arXiv Scraping\n",
    "\n",
    "In this section, we scrape the arXiv website for papers in the category \"cs.CV\" (Computer Vision), \"stat.ML\" / \"cs.LG\" (Machine Learning) and \"cs.AI\" (Artificial Intelligence). The papers are then saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a list of keywords that we will use to query the arXiv API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"self-supervised learning\\\"\",\n",
    "    \"\\\"representation learning\\\"\",\n",
    "    \"\\\"image generation\\\"\",\n",
    "    \"\\\"object detection\\\"\",\n",
    "    \"\\\"transfer learning\\\"\",\n",
    "    \"\\\"transformers\\\"\",\n",
    "    \"\\\"adversarial training\",\n",
    "    \"\\\"generative adversarial networks\\\"\",\n",
    "    \"\\\"model compressions\\\"\",\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"few-shot learning\\\"\",\n",
    "    \"\\\"natural language\\\"\",\n",
    "    \"\\\"graph\\\"\",\n",
    "    \"\\\"colorization\\\"\",\n",
    "    \"\\\"depth estimation\\\"\",\n",
    "    \"\\\"point cloud\\\"\",\n",
    "    \"\\\"structured data\\\"\",\n",
    "    \"\\\"optical flow\\\"\",\n",
    "    \"\\\"reinforcement learning\\\"\",\n",
    "    \"\\\"super resolution\\\"\",\n",
    "    \"\\\"attention\\\"\",\n",
    "    \"\\\"tabular\\\"\",\n",
    "    \"\\\"unsupervised learning\\\"\",\n",
    "    \"\\\"semi-supervised learning\\\"\",\n",
    "    \"\\\"explainable\\\"\",\n",
    "    \"\\\"radiance field\\\"\",\n",
    "    \"\\\"decision tree\\\"\",\n",
    "    \"\\\"time series\\\"\",\n",
    "    \"\\\"molecule\\\"\",\n",
    "    \"\\\"large language models\\\"\",\n",
    "    \"\\\"llms\\\"\",\n",
    "    \"\\\"language models\\\"\",\n",
    "    \"\\\"image classification\\\"\",\n",
    "    \"\\\"document image classification\\\"\",\n",
    "    \"\\\"encoder\\\"\",\n",
    "    \"\\\"decoder\\\"\",\n",
    "    \"\\\"multimodal\\\"\",\n",
    "    \"\\\"multimodal deep learning\\\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Afterwards, we define a function that creates a search object using the given query. It sets the maximum number of results for each category to 6000 and sorts them by the last updated date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(num_retries=20, page_size=500)\n",
    "\n",
    "\n",
    "def query_with_keywords(query) -> tuple:\n",
    "    \"\"\"\n",
    "    Query the arXiv API for research papers based on a specific query and filter results by selected categories.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be used for fetching research papers from arXiv.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - terms, titles, and abstracts of the filtered research papers.\n",
    "        \n",
    "            terms (list): A list of lists, where each inner list contains the categories associated with a research paper.\n",
    "            titles (list): A list of titles of the research papers.\n",
    "            abstracts (list): A list of abstracts (summaries) of the research papers.\n",
    "            urls (list): A list of URLs for the papers' detail page on the arXiv website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a search object with the query and sorting parameters.\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=6000,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists for terms, titles, abstracts, and urls.\n",
    "    terms = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "\n",
    "    # For each result in the search...\n",
    "    for res in tqdm(client.results(search), desc=query):\n",
    "        # Check if the primary category of the result is in the specified list.\n",
    "        if res.primary_category in [\"cs.CV\", \"stat.ML\", \"cs.LG\", \"cs.AI\"]:\n",
    "            # If it is, append the result's categories, title, summary, and url to their respective lists.\n",
    "            terms.append(res.categories)\n",
    "            titles.append(res.title)\n",
    "            abstracts.append(res.summary)\n",
    "            urls.append(res.entry_id)\n",
    "\n",
    "    # Return the four lists.\n",
    "    return terms, titles, abstracts, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 4744it [01:46, 44.43it/s]\n",
      "\"self-supervised learning\": 0it [00:02, ?it/s]\n",
      "\"representation learning\": 6000it [02:46, 36.14it/s]\n",
      "\"image generation\": 5105it [02:19, 36.50it/s]\n",
      "\"object detection\": 6000it [02:13, 44.91it/s]\n",
      "\"transfer learning\": 6000it [02:11, 45.54it/s]\n",
      "\"transformers\": 4501it [01:49, 44.67it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as iso-8859-2\n",
      "\"transformers\": 6000it [02:09, 46.47it/s]\n",
      "\"adversarial training: 0it [00:02, ?it/s]\n",
      "\"generative adversarial networks\": 6000it [02:03, 48.47it/s]\n",
      "\"model compressions\": 1154it [00:22, 50.70it/s]\n",
      "\"image segmentation\": 4744it [01:25, 55.55it/s]\n",
      "\"few-shot learning\": 0it [00:03, ?it/s]\n",
      "\"natural language\": 6000it [02:14, 44.74it/s]\n",
      "\"graph\": 6000it [02:08, 46.77it/s]\n",
      "\"colorization\": 6000it [02:11, 45.55it/s]\n",
      "\"depth estimation\": 2039it [00:45, 45.07it/s]\n",
      "\"point cloud\": 6000it [02:09, 46.49it/s]\n",
      "\"structured data\": 2810it [01:34, 29.62it/s]\n",
      "\"optical flow\": 2087it [00:35, 58.03it/s]\n",
      "\"reinforcement learning\": 6000it [02:01, 49.44it/s]\n",
      "\"super resolution\": 4317it [01:31, 47.09it/s]\n",
      "\"attention\": 6000it [02:02, 48.87it/s]\n",
      "\"tabular\": 2668it [00:55, 48.06it/s]\n",
      "\"unsupervised learning\": 3546it [01:28, 40.13it/s]\n",
      "\"semi-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"explainable\": 6000it [02:34, 38.83it/s]\n",
      "\"radiance field\": 1745it [00:37, 46.38it/s]\n",
      "\"decision tree\": 3434it [01:11, 47.70it/s]\n",
      "\"time series\": 6000it [02:07, 47.19it/s]\n",
      "\"molecule\": 6000it [02:01, 49.20it/s]\n",
      "\"large language models\": 6000it [02:13, 45.11it/s]\n",
      "\"llms\": 6000it [01:57, 51.09it/s]\n",
      "\"language models\": 6000it [02:21, 42.29it/s]\n",
      "\"image classification\": 6000it [02:11, 45.53it/s]\n",
      "\"document image classification\": 29it [00:03,  8.85it/s]\n",
      "\"encoder\": 6000it [02:03, 48.73it/s]\n",
      "\"decoder\": 6000it [02:07, 47.18it/s]\n",
      "\"multimodal\": 6000it [02:04, 48.16it/s]\n",
      "\"multimodal deep learning\": 134it [00:04, 28.86it/s]\n"
     ]
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_abstracts = []\n",
    "all_terms = []\n",
    "all_urls = []\n",
    "\n",
    "for query in query_keywords:\n",
    "    terms, titles, abstracts, urls = query_with_keywords(query)\n",
    "    all_titles.extend(titles)\n",
    "    all_abstracts.extend(abstracts)\n",
    "    all_terms.extend(terms)\n",
    "    all_urls.extend(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a pandas.DataFrame object to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data_indexed = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data_indexed.reset_index(inplace=True)\n",
    "arxiv_data_indexed.rename(columns = {'index':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "\n",
    "In this part, we preprocess the data collected in the previous section. We start by removing duplicates and then we clean the text by removing punctuation, stopwords and lemmatizing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aldir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aldir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aldir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</td>\n",
       "      <td>Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20924v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation</td>\n",
       "      <td>Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20838v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Solar Filaments Detection using Active Contours Without Edges</td>\n",
       "      <td>In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.</td>\n",
       "      <td>[cs.CV, astro-ph.IM, astro-ph.SR, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20749v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</td>\n",
       "      <td>While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2409.05393v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</td>\n",
       "      <td>Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2408.07343v4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       titles  \\\n",
       "0  HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization   \n",
       "1                                                           Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation   \n",
       "2                                                               Solar Filaments Detection using Active Contours Without Edges   \n",
       "3                                                    TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation   \n",
       "4                                             Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstracts  \\\n",
       "0                                                                                                            Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.   \n",
       "3                                                                                                                                                                                                                                            While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.   \n",
       "4  Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.   \n",
       "\n",
       "                                             terms  \\\n",
       "0                                   [cs.CV, cs.AI]   \n",
       "1                            [cs.CV, cs.AI, cs.LG]   \n",
       "2  [cs.CV, astro-ph.IM, astro-ph.SR, cs.AI, cs.LG]   \n",
       "3                                          [cs.CV]   \n",
       "4                                          [cs.CV]   \n",
       "\n",
       "                                urls  \n",
       "0  http://arxiv.org/abs/2412.20924v1  \n",
       "1  http://arxiv.org/abs/2412.20838v1  \n",
       "2  http://arxiv.org/abs/2412.20749v1  \n",
       "3  http://arxiv.org/abs/2409.05393v2  \n",
       "4  http://arxiv.org/abs/2408.07343v4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</td>\n",
       "      <td>Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20924v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation</td>\n",
       "      <td>Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20838v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Solar Filaments Detection using Active Contours Without Edges</td>\n",
       "      <td>In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.</td>\n",
       "      <td>[cs.CV, astro-ph.IM, astro-ph.SR, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2412.20749v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</td>\n",
       "      <td>While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2409.05393v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</td>\n",
       "      <td>Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2408.07343v4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                       titles  \\\n",
       "0  HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization   \n",
       "1                                                           Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation   \n",
       "2                                                               Solar Filaments Detection using Active Contours Without Edges   \n",
       "3                                                    TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation   \n",
       "4                                             Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstracts  \\\n",
       "0                                                                                                            Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.   \n",
       "3                                                                                                                                                                                                                                            While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.   \n",
       "4  Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.   \n",
       "\n",
       "                                             terms  \\\n",
       "0                                   [cs.CV, cs.AI]   \n",
       "1                            [cs.CV, cs.AI, cs.LG]   \n",
       "2  [cs.CV, astro-ph.IM, astro-ph.SR, cs.AI, cs.LG]   \n",
       "3                                          [cs.CV]   \n",
       "4                                          [cs.CV]   \n",
       "\n",
       "                                urls  \n",
       "0  http://arxiv.org/abs/2412.20924v1  \n",
       "1  http://arxiv.org/abs/2412.20838v1  \n",
       "2  http://arxiv.org/abs/2412.20749v1  \n",
       "3  http://arxiv.org/abs/2409.05393v2  \n",
       "4  http://arxiv.org/abs/2408.07343v4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83939 rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_data_indexed)} rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world data is noisy. One of the most commonly observed source of noise is data duplication. Here we notice that our initial dataset has got about 20k duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23376 duplicate titles.\n"
     ]
    }
   ],
   "source": [
    "total_duplicate_titles = sum(arxiv_data_indexed[\"titles\"].duplicated())\n",
    "print(f\"There are {total_duplicate_titles} duplicate titles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before proceeding further, we drop these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60563 rows in the deduplicated dataset.\n"
     ]
    }
   ],
   "source": [
    "arxiv_data_indexed = arxiv_data_indexed[~arxiv_data_indexed[\"titles\"].duplicated()]\n",
    "print(f\"There are {len(arxiv_data_indexed)} rows in the deduplicated dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Hopsworks Feature Store\n",
    "\n",
    "Before creating a feature group, we need to connect to Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hopsworks API key from .env file or secrets.toml file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    HOPSWORKS_API_KEY = os.environ['HOPSWORKS_API_KEY']\n",
    "    # HOPSWORKS_API_KEY = st.secrets.HOPSWORKS.HOPSWORKS_API_KEY\n",
    "except:\n",
    "    raise Exception('Set environment variable HOPSWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'hsfs' has no attribute 'hopsworks_udf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhopsworks\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import hsfs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#     print(f\"An error occurred: {e}\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aldir\\anaconda3\\envs\\venvpaper\\Lib\\site-packages\\hopsworks\\__init__.py:61\u001b[0m\n\u001b[0;32m     58\u001b[0m _secrets_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m _project_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m udf \u001b[38;5;241m=\u001b[39m \u001b[43mhsfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhopsworks_udf\u001b[49m\u001b[38;5;241m.\u001b[39mudf\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhw_formatwarning\u001b[39m(message, category, filename, lineno, line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(category\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, message)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'hsfs' has no attribute 'hopsworks_udf'"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "# import hsfs\n",
    "\n",
    "# try:\n",
    "#     project = hopsworks.login()\n",
    "#     # connection = project.connection(\n",
    "#     #     host='c.app.hopsworks.ai',\n",
    "#     #     project='paperrecommendation',\n",
    "#     #     api_key_value=HOPSWORKS_API_KEY,\n",
    "#     # )\n",
    "#     fs = project.get_feature_store()\n",
    "#     print(\"Connected to the Hopsworks Feature Store\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "    print(\"Connected to the Hopsworks project\")\n",
    "    \n",
    "    fs = project.get_feature_store()\n",
    "    print(\"Connected to the Hopsworks Feature Store\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature groups and uploading them to the Feature Store\n",
    "\n",
    "A feature group can be seen as a collection of conceptually related features. In this case, we will create 1 feature group representing the scientific paper information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m paper_info_fg \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_or_create_feature_group(\n\u001b[0;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpapers_info\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScientific papers info for recommendations.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     primary_key\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "paper_info_fg = fs.get_or_create_feature_group(\n",
    "    name=\"papers_info\",\n",
    "    version=1,\n",
    "    description=\"Scientific papers info for recommendations.\",\n",
    "    primary_key=['id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent, we need to populate it with its associated data using the `insert` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    paper_info_fg.insert(arxiv_data_indexed, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = [\n",
    "    {\"name\": \"id\", \"description\": \"Scientific paper IDs\"}, \n",
    "    {\"name\": \"titles\", \"description\": \"Scientific paper titles\"}, \n",
    "    {\"name\": \"abstracts\", \"description\": \"Scientific paper abstracts\"}, \n",
    "    {\"name\": \"terms\", \"description\": \"Scientific paper categories\"}, \n",
    "    {\"name\": \"urls\", \"description\": \"URLs to scientific paper detail pages\"}, \n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    paper_info_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature group is now accessible and searchable in the UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
