{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings\n",
    "\n",
    "This notebook contains the code to generate sentence embeddings using the pre-trained models from the sentence-transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / 'data'\n",
    "PATH_SENTENCES = Path.cwd().parent / 'models/sentences'\n",
    "PATH_EMBEDDINGS = Path.cwd().parent / 'models/embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization</td>\n",
       "      <td>Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "      <td>http://arxiv.org/abs/2412.20924v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation</td>\n",
       "      <td>Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2412.20838v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Solar Filaments Detection using Active Contours Without Edges</td>\n",
       "      <td>In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.</td>\n",
       "      <td>['cs.CV', 'astro-ph.IM', 'astro-ph.SR', 'cs.AI', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2412.20749v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</td>\n",
       "      <td>While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2409.05393v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</td>\n",
       "      <td>Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2408.07343v4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       titles  \\\n",
       "0  HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization   \n",
       "1                                                           Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation   \n",
       "2                                                               Solar Filaments Detection using Active Contours Without Edges   \n",
       "3                                                    TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation   \n",
       "4                                             Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstracts  \\\n",
       "0                                                                                                            Tissue semantic segmentation is one of the key tasks in computational\\npathology. To avoid the expensive and laborious acquisition of pixel-level\\nannotations, a wide range of studies attempt to adopt the class activation map\\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\\nsegmentation. However, CAM-based methods are prone to suffer from\\nunder-activation and over-activation issues, leading to poor segmentation\\nperformance. To address this problem, we propose a novel weakly-supervised\\nsemantic segmentation framework for histopathological images based on\\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\\nSpecifically, synthesized histopathological images with pixel-level masks are\\ngenerated for fully-supervised model training, where two synthesis strategies\\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\\nBesides, an image filtering module is developed to guarantee the authenticity\\nof the synthesized images. In order to further avoid the model overfitting to\\nthe occasional synthesis artifacts, we additionally propose a novel\\nself-supervised consistency regularization, which enables the real images\\nwithout segmentation masks to supervise the training of the segmentation model.\\nBy integrating the proposed techniques, the HisynSeg framework successfully\\ntransforms the weakly-supervised semantic segmentation problem into a\\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\\nresults on three datasets prove that the proposed method achieves a\\nstate-of-the-art performance. Code is available at\\nhttps://github.com/Vison307/HisynSeg.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Accurate segmentation of wind turbine blade (WTB) images is critical for\\neffective assessments, as it directly influences the performance of automated\\ndamage detection systems. Despite advancements in large universal vision\\nmodels, these models often underperform in domain-specific tasks like WTB\\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\\nand propose a novel dual-space augmentation strategy that integrates both\\nimage-level and latent-space augmentations. The image-space augmentation is\\nachieved through linear interpolation between image pairs, while the\\nlatent-space augmentation is accomplished by introducing a noise-based latent\\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\\nsurpassing current state-of-the-art methods in WTB image segmentation.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this article, an active contours without edges (ACWE)-based algorithm has\\nbeen proposed for the detection of solar filaments in H-alpha full-disk solar\\nimages. The overall algorithm consists of three main steps of image processing.\\nThese are image pre-processing, image segmentation, and image post-processing.\\nHere in the work, contours are initialized on the solar image and allowed to\\ndeform based on the energy function. As soon as the contour reaches the\\nboundary of the desired object, the energy function gets reduced, and the\\ncontour stops evolving. The proposed algorithm has been applied to few\\nbenchmark datasets and has been compared with the classical technique of object\\ndetection. The results analysis indicates that the proposed algorithm\\noutperforms the results obtained using the existing classical algorithm of\\nobject detection.   \n",
       "3                                                                                                                                                                                                                                            While large visual models (LVM) demonstrated significant potential in image\\nunderstanding, due to the application of large-scale pre-training, the Segment\\nAnything Model (SAM) has also achieved great success in the field of image\\nsegmentation, supporting flexible interactive cues and strong learning\\ncapabilities. However, SAM's performance often falls short in cross-domain and\\nfew-shot applications. Previous work has performed poorly in transferring prior\\nknowledge from base models to new applications. To tackle this issue, we\\npropose a task-adaptive auto-visual prompt framework, a new paradigm for\\nCross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature\\nFusion (MFF) was used for integrated feature extraction as prior knowledge.\\nBesides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module\\nto enable class-domain agnostic feature extraction and generate high-quality,\\nlearnable visual prompts. This significant advancement uses a unique generative\\napproach to prompts alongside a comprehensive model structure and specialized\\nprototype computation. While ensuring that the prior knowledge of SAM is not\\ndiscarded, the new branch disentangles category and domain information through\\nprototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across\\nfour cross-domain datasets demonstrate that our model outperforms the\\nstate-of-the-art CD-FSS approach, achieving an average accuracy improvement of\\n1.3\\% in the 1-shot setting and 11.76\\% in the 5-shot setting.   \n",
       "4  Although recent years have witnessed significant advancements in medical\\nimage segmentation, the pervasive issue of domain shift among medical images\\nfrom diverse centres hinders the effective deployment of pre-trained models.\\nMany Test-time Adaptation (TTA) methods have been proposed to address this\\nissue by fine-tuning pre-trained models with test data during inference. These\\nmethods, however, often suffer from less-satisfactory optimization due to\\nsuboptimal optimization direction (dictated by the gradient) and fixed\\nstep-size (predicated on the learning rate). In this paper, we propose the\\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\\nthe gradient direction and learning rate in the optimization procedure. Unlike\\nconventional TTA methods, which primarily optimize the pseudo gradient derived\\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\\nenables the model to excavate the similarities between different gradients and\\ncorrect the gradient direction to approximate the empirical gradient related to\\nthe current segmentation task. Additionally, we design a dynamic learning rate\\nbased on the cosine similarity between the pseudo and auxiliary gradients,\\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\\ntest data. Extensive experiments establish the effectiveness of the proposed\\ngradient alignment and dynamic learning rate and substantiate the superiority\\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\\nmedical image segmentation task. The code and weights of pre-trained source\\nmodels are available at https://github.com/Chen-Ziyang/GraTa.   \n",
       "\n",
       "                                                       terms  \\\n",
       "0                                         ['cs.CV', 'cs.AI']   \n",
       "1                                ['cs.CV', 'cs.AI', 'cs.LG']   \n",
       "2  ['cs.CV', 'astro-ph.IM', 'astro-ph.SR', 'cs.AI', 'cs.LG']   \n",
       "3                                                  ['cs.CV']   \n",
       "4                                                  ['cs.CV']   \n",
       "\n",
       "                                urls  \n",
       "0  http://arxiv.org/abs/2412.20924v1  \n",
       "1  http://arxiv.org/abs/2412.20838v1  \n",
       "2  http://arxiv.org/abs/2412.20749v1  \n",
       "3  http://arxiv.org/abs/2409.05393v2  \n",
       "4  http://arxiv.org/abs/2408.07343v4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(PATH_DATA_BASE / 'filtered_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence-transformers models\n",
    "\n",
    "#### What is a sentence-transformers model?\n",
    "\n",
    "It maps sentences & paragraphs to a N dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all-MiniLM-L6-v2\n",
    "\n",
    "MiniLM is a smaller variatn of the BERT model which has been designed to provide high-quality language understanding capabilities while being significantly smaller and more efficient. The \"`all-MiniLM-L6-v2`\" model refers to a specific configuration of teh MiniLM model.\n",
    "\n",
    "Here are some reasons why this model is a good choice for our use case:\n",
    "\n",
    "* Efficiency: MiniLM models are smaller and faster than full-size BERT models, which can be a major advantage if you're working on a project with limited computational resources or if you need to process large amounts of data quickly.\n",
    "\n",
    "* Performance: Despite their smaller size, MiniLM models often perform at a comparable level to full-size BERT models on a variety of NLP tasks. This means that you can often use a MiniLM model without sacrificing much in the way of performance. In fact, the `Performance Sentence Embeddings` metric which is the average performance on encoding sentences over 14 diverse tasks from different domains is `68.06` for the `all-MiniLM-L6-v2` model, which is very good to start with.\n",
    "\n",
    "* Ease of Use: If you're using a library like Hugging Face's Transformers, it can be relatively straightforward to load a pre-trained MiniLM model and fine-tune it for your specific task.\n",
    "\n",
    "* Lower Memory Requirements: Given its smaller size, MiniLM requires less memory for training and inference. This could be a crucial factor if you're working with limited hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Our feature we like to encode\n",
    "sentences = dataset['titles']\n",
    "\n",
    "# Features are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: HisynSeg: Weakly-Supervised Histopathological Image Segmentation via Image-Mixing Synthesis and Consistency Regularization\n",
      "Embedding dimension: 384\n",
      "Title length: 122\n",
      "\n",
      "Sentence: Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 65\n",
      "\n",
      "Sentence: Solar Filaments Detection using Active Contours Without Edges\n",
      "Embedding dimension: 384\n",
      "Title length: 61\n",
      "\n",
      "Sentence: TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 72\n",
      "\n",
      "Sentence: Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 79\n",
      "\n",
      "Sentence: A Large-scale Interpretable Multi-modality Benchmark for Facial Image Forgery Localization\n",
      "Embedding dimension: 384\n",
      "Title length: 90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "c = 0\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding dimension:\", len(embedding))\n",
    "    print(\"Title length:\", len(sentence))\n",
    "    print(\"\")\n",
    "    \n",
    "    if c >= 5:\n",
    "        break\n",
    "    c += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open(PATH_EMBEDDINGS / 'embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "    \n",
    "with open(PATH_SENTENCES / 'sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self Driving'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_you_like = input(\"Enter your topic of interest here ðŸ‘‡ \\n\")\n",
    "paper_you_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "cosine_scores = util.cos_sim(embeddings, model.encode(paper_you_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.5887],\n",
       "        [0.5686],\n",
       "        [0.5554],\n",
       "        [0.5535],\n",
       "        [0.5486]]),\n",
       "indices=tensor([[13715],\n",
       "        [ 5041],\n",
       "        [32691],\n",
       "        [36365],\n",
       "        [13906]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "top_similar_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features\n",
      "Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences\n",
      "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting\n",
      "Learning On-Road Visual Control for Self-Driving Vehicles with Auxiliary Tasks\n",
      "UniWorld: Autonomous Driving Pre-training via World Models\n"
     ]
    }
   ],
   "source": [
    "for i in top_similar_papers.indices:\n",
    "    print(sentences[i.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvpaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
