{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection process\n",
    "\n",
    "This notebook scrapes the arXiv website for papers in the category \"cs.CV\" (computer vision), \"stat.ML\" / \"cs.LG\" (Machine Learning) and \"cs.AI\" (Artificial Intelligence). The papers are then saved in the a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the arXiv website\n",
    "\n",
    "Let's start by defining a list of keywords that we will use to query the arXiv API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can added some keywords here to search for specific topics\n",
    "query_keywords = [\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"self-supervised learning\\\"\",\n",
    "    \"\\\"representation learning\\\"\",\n",
    "    \"\\\"image generation\\\"\",\n",
    "    \"\\\"object detection\\\"\",\n",
    "    \"\\\"transfer learning\\\"\",\n",
    "    \"\\\"transformers\\\"\",\n",
    "    \"\\\"adversarial training\",\n",
    "    \"\\\"generative adversarial networks\\\"\",\n",
    "    \"\\\"model compressions\\\"\",\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"few-shot learning\\\"\",\n",
    "    \"\\\"natural language\\\"\",\n",
    "    \"\\\"graph\\\"\",\n",
    "    \"\\\"colorization\\\"\",\n",
    "    \"\\\"depth estimation\\\"\",\n",
    "    \"\\\"point cloud\\\"\",\n",
    "    \"\\\"structured data\\\"\",\n",
    "    \"\\\"optical flow\\\"\",\n",
    "    \"\\\"reinforcement learning\\\"\",\n",
    "    \"\\\"super resolution\\\"\",\n",
    "    \"\\\"attention\\\"\",\n",
    "    \"\\\"tabular\\\"\",\n",
    "    \"\\\"unsupervised learning\\\"\",\n",
    "    \"\\\"semi-supervised learning\\\"\",\n",
    "    \"\\\"explainable\\\"\",\n",
    "    \"\\\"radiance field\\\"\",\n",
    "    \"\\\"decision tree\\\"\",\n",
    "    \"\\\"time series\\\"\",\n",
    "    \"\\\"molecule\\\"\",\n",
    "    \"\\\"large language models\\\"\",\n",
    "    \"\\\"llms\\\"\",\n",
    "    \"\\\"language models\\\"\",\n",
    "    \"\\\"image classification\\\"\",\n",
    "    \"\\\"document image classification\\\"\",\n",
    "    \"\\\"encoder\\\"\",\n",
    "    \"\\\"decoder\\\"\",\n",
    "    \"\\\"multimodal\\\"\",\n",
    "    \"\\\"multimodal deep learning\\\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we define a function that creates a search object using the given query. It sets the maximum number of results for each category to 6000 and sorts them by the last updated date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(num_retries=20, page_size=500)\n",
    "\n",
    "\n",
    "def query_with_keywords(query) -> tuple:\n",
    "    \"\"\"\n",
    "    Query the arXiv API for research papers based on a specific query and filter results by selected categories.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be used for fetching research papers from arXiv.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - terms, titles, and abstracts of the filtered research papers.\n",
    "        \n",
    "            terms (list): A list of lists, where each inner list contains the categories associated with a research paper.\n",
    "            titles (list): A list of titles of the research papers.\n",
    "            abstracts (list): A list of abstracts (summaries) of the research papers.\n",
    "            urls (list): A list of URLs for the papers' detail page on the arXiv website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a search object with the query and sorting parameters.\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=6000,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists for terms, titles, abstracts, and urls.\n",
    "    terms = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "\n",
    "    # For each result in the search...\n",
    "    for res in tqdm(client.results(search), desc=query):\n",
    "        # Check if the primary category of the result is in the specified list.\n",
    "        if res.primary_category in [\"cs.CV\", \"stat.ML\", \"cs.LG\", \"cs.AI\"]:\n",
    "            # If it is, append the result's categories, title, summary, and url to their respective lists.\n",
    "            terms.append(res.categories)\n",
    "            titles.append(res.title)\n",
    "            abstracts.append(res.summary)\n",
    "            urls.append(res.entry_id)\n",
    "\n",
    "    # Return the four lists.\n",
    "    return terms, titles, abstracts, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 4744it [01:37, 48.73it/s]\n",
      "\"self-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"representation learning\": 6000it [02:06, 47.32it/s]\n",
      "\"image generation\": 5105it [02:52, 29.53it/s]\n",
      "\"object detection\": 6000it [02:16, 43.83it/s]\n",
      "\"transfer learning\": 6000it [03:04, 32.55it/s]\n",
      "\"transformers\": 4501it [01:43, 49.91it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as iso-8859-2\n",
      "\"transformers\": 6000it [01:59, 50.11it/s]\n",
      "\"adversarial training: 0it [00:02, ?it/s]\n",
      "\"generative adversarial networks\": 6000it [01:44, 57.57it/s]\n",
      "\"model compressions\": 1154it [00:26, 44.14it/s]\n",
      "\"image segmentation\": 4744it [01:18, 60.53it/s]\n",
      "\"few-shot learning\": 0it [00:03, ?it/s]\n",
      "\"natural language\": 6000it [02:36, 38.38it/s]\n",
      "\"graph\": 6000it [02:28, 40.49it/s]\n",
      "\"colorization\": 6000it [02:15, 44.19it/s]\n",
      "\"depth estimation\": 2039it [00:46, 43.83it/s]\n",
      "\"point cloud\": 6000it [02:09, 46.45it/s]\n",
      "\"structured data\": 2810it [01:03, 44.46it/s]\n",
      "\"optical flow\": 2087it [00:57, 36.44it/s]\n",
      "\"reinforcement learning\": 6000it [02:41, 37.07it/s]\n",
      "\"super resolution\": 4317it [01:38, 43.86it/s]\n",
      "\"attention\": 6000it [02:29, 40.16it/s]\n",
      "\"tabular\": 2668it [01:00, 44.27it/s]\n",
      "\"unsupervised learning\": 3546it [01:22, 43.02it/s]\n",
      "\"semi-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"explainable\": 6000it [02:25, 41.24it/s]\n",
      "\"radiance field\": 1745it [00:42, 40.99it/s]\n",
      "\"decision tree\": 3434it [01:13, 46.54it/s]\n",
      "\"time series\": 6000it [02:38, 37.87it/s]\n",
      "\"molecule\": 6000it [02:20, 42.77it/s]\n",
      "\"large language models\": 6000it [03:30, 28.45it/s]\n",
      "\"llms\": 6000it [02:00, 49.93it/s]\n",
      "\"language models\": 6000it [02:56, 34.06it/s]\n",
      "\"image classification\": 6000it [02:12, 45.12it/s]\n",
      "\"document image classification\": 29it [00:03,  9.54it/s]\n",
      "\"encoder\": 6000it [02:03, 48.53it/s]\n",
      "\"decoder\": 6000it [02:12, 45.27it/s]\n",
      "\"multimodal\": 6000it [01:54, 52.36it/s]\n",
      "\"multimodal deep learning\": 134it [00:05, 26.20it/s]\n"
     ]
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_abstracts = []\n",
    "all_terms = []\n",
    "all_urls = []\n",
    "\n",
    "for query in query_keywords:\n",
    "    terms, titles, abstracts, urls = query_with_keywords(query)\n",
    "    all_titles.extend(titles)\n",
    "    all_abstracts.extend(abstracts)\n",
    "    all_terms.extend(terms)\n",
    "    all_urls.extend(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we export the DataFrame to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.to_csv(PATH_DATA_BASE / \"data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
